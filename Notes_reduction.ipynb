{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Serial result:  65536.0\n"
    }
   ],
   "source": [
    "# Serial reduction (Dot product)\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "N = 2**16\n",
    "\n",
    "def dot(u,v):\n",
    "    n = u.shape[0]\n",
    "    accum = 0\n",
    "    for i in range(n):\n",
    "        accum += u[i] * v[i]\n",
    "    return accum\n",
    "\n",
    "def main():\n",
    "\tu = np.ones(N, dtype = np.float32) \n",
    "\tv = np.ones(N, dtype = np.float32) \n",
    "\n",
    "\taccum = 0 \n",
    "\tfor i in range(N):\n",
    "\t\taccum += u[i]*v[i] \n",
    "\tprint(\"Serial result: \", accum)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'debug'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-f7806764dcfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mTPB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdot_kernel\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0md_accum\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0md_u\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0md_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numba/cuda/decorators.py\u001b[0m in \u001b[0;36mjit\u001b[0;34m(func_or_sig, argtypes, device, inline, bind, link, debug, **kws)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENABLE_CUDASIM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 return FakeCUDAKernel(func_or_sig, device=device, fastmath=fastmath,\n\u001b[0;32m---> 75\u001b[0;31m                                       debug=debug)\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mjitdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_or_sig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'debug'"
     ]
    }
   ],
   "source": [
    "# Reductions\n",
    "# Issues with accessing global memory during parallel computations\n",
    "# Must control order or execution\n",
    "# Use \"atomic operations\" to elimiate race conditions: \"read-increment-write\" and prevent other threads from reading values\n",
    "# Other atomic operations\n",
    "# cuda.atomic.max() and cuda.atomic.min()\n",
    "\n",
    "import os\n",
    "os.environ['NUMBA_ENABLE_CUDASIM'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda , float32\n",
    "\n",
    "N = 2**16\n",
    "TPB = 32\n",
    "\n",
    "@cuda.jit\n",
    "def dot_kernel( d_accum , d_u , d_v):\n",
    "    i = cuda.grid(1)\n",
    "    n = d_u.shape[0]\n",
    "\n",
    "    if i >= n:\n",
    "       return\n",
    "\n",
    "    w = d_u[i]*d_v[i]\n",
    "    cuda.atomic.add( d_accum , 0, w) # New addition: But this basically creates a version slower than serial\n",
    "\n",
    "def dot(u, v):\n",
    "    n = u.shape[0]\n",
    "    accum = np.zeros (1, dtype = np.float32)\n",
    "\n",
    "    d_u = cuda.to_device(u)\n",
    "    d_v = cuda.to_device(v)\n",
    "    d_accum = cuda.to_device( accum )\n",
    "\n",
    "    gridDim = (n + TPB - 1)//TPB\n",
    "    blockDim = TPB\n",
    "\n",
    "    dot_kernel[gridDim,blockDim]( d_accum , d_u , d_v)\n",
    "\n",
    "    return d_accum.copy_to_host()[0]\n",
    "\n",
    "def main():\n",
    "\tu = np.ones(N, dtype = np.float32) \n",
    "\tv = np.ones(N, dtype = np.float32) \n",
    "\n",
    "\tfor j in range(8):\n",
    "\t\taccum = dot(u, v) \n",
    "\t\tprint(\"Naive parallel result: \", accum)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better to create shared memory arrays for parallel reductions\n",
    "# Split reduction into blocks that can be summed together at the end\n",
    "\n",
    "import os\n",
    "os.environ['NUMBA_ENABLE_CUDASIM'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda , float32\n",
    "\n",
    "N = 2**16\n",
    "TPB = 512\n",
    "\n",
    "@cuda.jit('void(f4[:],f4[:],f4[:])')\n",
    "def dot_kernel(d_accum, d_u, d_v):\n",
    "\ti = cuda.grid(1)\n",
    "\ttIdx = cuda.threadIdx.x\n",
    "\tn = d_u.shape[0]\n",
    "\tsh_w = cuda.shared.array(shape = TPB, dtype = float32) #establish shared array\n",
    "\tsh_w[tIdx] = 0 #initialize shared array to zero\n",
    "\n",
    "\tif i < n: #bounds check\n",
    "\t\tsh_w[tIdx] = d_u[i]*d_v[i] #store element product in shared array\n",
    "\tcuda.syncthreads() #make sure all element products are stored before summing\n",
    "\n",
    "\tif tIdx == 0: #assign thread 0 to compute the block sum\n",
    "\t\tblock_sum = 0\n",
    "\t\tfor j in range(cuda.blockDim.x):\n",
    "\t\t\tblock_sum += sh_w[tIdx]\n",
    "\t\tcuda.atomic.add(d_accum , 0, block_sum)\n",
    "\n",
    "def dot(u, v):\n",
    "\td_u = cuda.to_device(u)\n",
    "\td_v = cuda.to_device(v)\n",
    "\tgridDim = int(np.ceil(u.size/TPB))\n",
    "\tblockDim = TPB\n",
    "\taccum = np.zeros(1, dtype = np.float32)\n",
    "\td_accum = cuda.to_device(accum)\n",
    "\n",
    "\tdot_kernel[gridDim, blockDim](d_accum, d_u, d_v)\n",
    "\n",
    "\treturn d_accum.copy_to_host()[0]\n",
    "\n",
    "def main():\n",
    "\tu = np.ones(N, dtype = np.float32) \n",
    "\tv = np.ones(N, dtype = np.float32) \n",
    "\n",
    "\tfor j in range(8):\n",
    "\t\taccum = dot(u, v) \n",
    "\t\tprint(\"Naive parallel result: \", accum)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "# A more complex and efficient version is available in the ch7_atomic reductions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numba built in reduction\n",
    "# @cuda.reduce\n",
    "# This is a sum reduction\n",
    "\n",
    "import os\n",
    "os.environ['NUMBA_ENABLE_CUDASIM'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda , float32\n",
    "\n",
    "N = 2**16\n",
    "TPB = 1024\n",
    "\n",
    "@cuda.reduce # new decorator to implement reduction\n",
    "def reduction (u, v): # function definition: compare values within w (u and v)\n",
    "    return u + v # If we want to do a max reduction max(u,v) instead\n",
    "\n",
    "@cuda.jit('void (f4 [:] , f4 [:] , f4 [:])')\n",
    "def dot_kernel(d_w , d_u , d_v):\n",
    "    i = cuda.grid(1)\n",
    "    n = d_u.shape [0]\n",
    "\n",
    "    if i >= n:\n",
    "        return\n",
    "\n",
    "    d_w[i] = d_u[i]*d_v[i]\n",
    "\n",
    "def dot(u, v):\n",
    "    n = u.shape [0]\n",
    "\n",
    "    d_u = cuda.to_device(u)\n",
    "    d_v = cuda.to_device(v)\n",
    "    d_w = cuda.device_array(n, dtype = np.float32)\n",
    "\n",
    "    gridDim = int( np.ceil (n/TPB))\n",
    "    blockDim = TPB\n",
    "    dot_kernel[gridDim ,blockDim ](d_w , d_u , d_v)\n",
    "\n",
    "    w = reduction(d_w) # function call: pass in array that you want to be reducted (this is odd)\n",
    "    return w\n",
    "\n",
    "def main():\n",
    "\tu = np.ones(N, dtype = np.float32) \n",
    "\tv = np.ones(N, dtype = np.float32) \n",
    "\n",
    "\tfor j in range(8):\n",
    "\t\taccum = dot(u, v) \n",
    "\t\tprint(\"Naive parallel result: \", accum)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}